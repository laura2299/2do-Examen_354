{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7MvmcisPrZvP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar los datos de entrada\n",
        "X_train = X_train / np.max(X_train)\n",
        "X_test = X_test / np.max(X_train)\n"
      ],
      "metadata": {
        "id": "We_LkQARrldw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Paso 3: Definir las funciones de activación y la función de pérdida\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    return -np.mean(y_true * np.log(y_pred))\n"
      ],
      "metadata": {
        "id": "UUV2LXGkruML"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicializar los parámetros de la red neuronal\n",
        "#input size representa el número de características en los datos de entrada. \n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 5\n",
        "output_size = len(np.unique(y_train))\n",
        "\n",
        "#W1 es una matriz de pesos que conecta las neuronas de la capa de entrada con las neuronas de la capa oculta.\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros(hidden_size)\n",
        "#W2 es una matriz de pesos que conecta las neuronas de la capa oculta con las neuronas de la capa de salida\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros(output_size)\n"
      ],
      "metadata": {
        "id": "c1Whfxi3sJjn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenar la red neuronal\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    z1 = np.dot(X_train, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "\n",
        "    # Backpropagation\n",
        "    d2 = a2 - np.eye(output_size)[y_train]\n",
        "    dW2 = np.dot(a1.T, d2)\n",
        "    db2 = np.sum(d2, axis=0)\n",
        "    d1 = np.dot(d2, W2.T) * sigmoid_derivative(z1)\n",
        "    dW1 = np.dot(X_train.T, d1)\n",
        "    db1 = np.sum(d1, axis=0)\n",
        "\n",
        "    # Actualizar los parámetros\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # Calcular la pérdida en cada epoch\n",
        "    loss = cross_entropy(np.eye(output_size)[y_train], a2)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "print(\"Entrenamiento finalizado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HybX3zO2sQqk",
        "outputId": "a64830f7-81ea-43bb-b536-7a5d71219a57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 1.3205\n",
            "Epoch 100: Loss = 1.2883\n",
            "Epoch 200: Loss = 0.4662\n",
            "Epoch 300: Loss = 0.3638\n",
            "Epoch 400: Loss = 0.3688\n",
            "Epoch 500: Loss = 0.4016\n",
            "Epoch 600: Loss = 0.3813\n",
            "Epoch 700: Loss = 0.3756\n",
            "Epoch 800: Loss = 0.3971\n",
            "Epoch 900: Loss = 0.3764\n",
            "Entrenamiento finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluar el rendimiento del modelo en el conjunto de prueba\n",
        "\n",
        "\n",
        "# Forward propagation en el conjunto de prueba\n",
        "z1_test = np.dot(X_test, W1) + b1\n",
        "a1_test = sigmoid(z1_test)\n",
        "z2_test = np.dot(a1_test, W2) + b2\n",
        "a2_test = softmax(z2_test)\n",
        "\n",
        "# Obtener las predicciones\n",
        "predictions = np.argmax(a2_test, axis=1)\n",
        "\n",
        "# Calcular la exactitud\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Exactitud en el conjunto de prueba: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sfoPRYYsfn7",
        "outputId": "90dae6d7-fb2a-4ff3-a339-bdf08ffa53b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactitud en el conjunto de prueba: 0.36666666666666664\n"
          ]
        }
      ]
    }
  ]
}